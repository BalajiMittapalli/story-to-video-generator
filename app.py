from typing import List, Optional
from langchain_google_genai import GoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain

from dotenv import load_dotenv
from gtts import gTTS
import os
import requests
import json
import time

load_dotenv()


REPLICATE_API_KEY = os.getenv("REPLICATE_API_KEY")
REPLICATE_API_URL = "https://api.replicate.com/v1/predictions"
REPLICATE_MODEL = "ac732df83cea7fff18b8472768c88ad041fa750ff7682a21affe81863cbe77e4"
IMAGE_HEIGHT = 768
IMAGE_WIDTH = 1024
GENERATIONS_DIR = "generations"


llm = GoogleGenerativeAI(
    model="gemini-1.5-flash",
    api_key=os.getenv("GEMINI_API_KEY"),
    temperature=0.5,
)

prompt_template = PromptTemplate(
    input_variables=["name", "story"],
    template="""
                
                You are a professional story writer for short form video content where the video will be created with a minimum of 10 scenes. You write the scenes for the story given by the user. The video that will be generated will comprise of images with animations and audio telling the story.

                The scene will be an array which will consist of image_prompt and audio_text of all the scenes like a json file of each scene.
                You need to write the audio_text of the scene in such a way that it will be a continuation of the previous scene audio_text so that the story will be continuous. The images need not be continous but should atleast be a little related to the scene.

                The scene will consist of a image with small animations like zoom in, zoom out, pan, etc. The audio will be a voice over of the text of the scene.
                The image that will be displayed will be generated by ai with the image_prompt from the scene. The audio will be generated from the audio_text from the scene.

                Each scene should be too long and each scene will be of the length of the audio_text. That is you need to generate the audio_text of each scene with 10-15 words and not more, so structure the scenes accordingly.

                You as the scene writer for the story needs to write a description for the image to be generated by ai and also you need to wite short audio overlays for the scene being delivered at the moment.
                So you need to think about the how many scenes should be there and how to structure them properly and how you need to write the image_prompt for the scene and how you need to write the audio_text for the scene.

                The output should be a json object with the scenes array which will consist of image_prompt and audio_text of all the scenes.
                
                Ensure the response is valid JSON and can be parsed by Python's json.loads() function.
                Do not include any additional text outside the JSON object.
                

                name of the story given by the user: {name}
                story given by the user: {story}
                Answer:""")


class Scene:
    def __init__(self):
        self.index: int = 0
        self.image_prompt: str = ""
        self.audio_text: str = ""
        self.image_file: Optional[str] = None
        self.audio_file: Optional[str] = None


class Project:
    def __init__(self, name: str, story: str, scenes: List[Scene]):
        self.name = name
        self.story = story
        self.scenes = scenes

    @property
    def project_dir(self):
        return os.path.join(GENERATIONS_DIR, self.name.replace(" ", "-"))


def create_scenes(prompt_template):
    chain = LLMChain(llm=llm, prompt=prompt_template)
    story = "A boy sleeping on his bed at night wanted to drink water, so he went to fridge and while taking water from the fridge something fell on top of him, so he ran to his bed but felt that something maybe sleeping in his sheets. so he went under the bed to sleep but then there is a hgost there, and then he suddenly woke from his dream and realised that it was just a dream."
    response = chain.invoke({
        "name": "fear at night",
                "story": story
    })

    raw_response = response['text'].strip()

    if raw_response.startswith("```json"):
        raw_response = raw_response[7:-3].strip()

    try:
        response_json = json.loads(raw_response)
        scenes_data = response_json.get("scenes", [])

        scenes = []
        for idx, scene_data in enumerate(scenes_data, start=1):
            scene = Scene()
            scene.index = idx
            scene.image_prompt = scene_data.get("image_prompt", "")
            scene.audio_text = scene_data.get("audio_text", "")
            scenes.append(scene)

        project = Project(
            name="fear at night",
            story=story,
            scenes=scenes
        )

        print(f"Created project with {len(scenes)} scenes:")
        for scene in scenes:
            print(f"Scene {scene.index}:")
            print(f"Image prompt: {scene.image_prompt}")
            print(f"Audio text: {scene.audio_text}\n")
        # print(project.scenes)

        return project

    except json.JSONDecodeError as e:
        print(f"JSON Decode Error: {e}")
    except Exception as e:
        print(f"Error processing response: {e}")


def poll_for_completion(prediction_id: str) -> Optional[str]:
    headers = {
        'Authorization': 'Bearer ' + REPLICATE_API_KEY,
        'Content-Type': 'application/json',
    }

    while True:
        try:
            response = requests.get(
                f"{REPLICATE_API_URL}/{prediction_id}",
                headers=headers
            )

            if response.status_code == 200:
                prediction = response.json()
                status = prediction.get("status")

                if status == "succeeded":
                    return prediction.get("output", [None])[0]
                elif status in ["failed", "canceled"]:
                    print(f"Prediction failed or was canceled: {prediction}")
                    return None
                else:
                    print(f"Prediction status: {
                          status}. Polling again in 5 seconds...")
                    time.sleep(5)
            else:
                print(f"Error polling prediction: {response.status_code}")
                print(response.text)
                return None

        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            return None


def create_images(project: Project):
    headers = {
        'Authorization': 'Bearer ' + REPLICATE_API_KEY,
        'Content-Type': 'application/json',
    }

    images_dir = os.path.join(project.project_dir, "images")
    os.makedirs(images_dir, exist_ok=True)

    for scene in project.scenes:
        if not scene.image_prompt:
            continue

        body = {
            "version": REPLICATE_MODEL,
            "input": {
                "width": IMAGE_WIDTH,
                "height": IMAGE_HEIGHT,
                "prompt": scene.image_prompt,
                "scheduler": "K_EULER",
                "num_outputs": 1,
                "guidance_scale": 7.5,
                "num_inference_steps": 50
            }
        }

        try:
            request = requests.post(
                url=REPLICATE_API_URL,
                headers=headers,
                json=body
            )
            response.raise_for_status()
            response = request.json()
            prediction_id = response.get("id")

            if output_url := poll_for_completion(prediction_id):
                image_path = os.path.join(images_dir, f"{scene.index}.png")
                img_response = requests.get(output_url)
                img_response.raise_for_status()

                with open(image_path, "wb") as f:
                    f.write(img_response.content)

                scene.image_file = image_path
                print(f"Generated image for scene {scene.index}")
            else:
                print("No prediction ID found in the response.")

        except Exception as e:
            print(f"Error generating image for scene {scene.index}: {str(e)}")


def create_audio(project: Project):
    audio_dir = os.path.join(project.project_dir, "audio")
    os.makedirs(audio_dir, exist_ok=True)

    for scene in project.scenes:
        if not scene.audio_text:
            continue

        try:
            filepath = os.path.join(audio_dir, f"{scene.index}.mp3")
            tts = gTTS(text=scene.audio_text, lang='en', slow=False)
            tts.save(filepath)
            scene.audio_file = filepath
            print(f"Generated audio for scene {scene.index}")
        except Exception as e:
            print(f"Error generating audio for scene {scene.index}: {str(e)}")  


def image_to_video_with_captions_overlay_and_audio():
    pass


def merge_video_scenes():
    pass


def main():
    project = Project(
        name="My Story",
        story="A story about a boy and a car.",
        scenes=[
            # Scene(index=0, imagePrompt="a boy taking water bottle from fridge at night",
            #       audioText="", imageFile=None, audioFile=None),
            Scene(index=1, imagePrompt="a futuristic city at sunset",
                  audioText="", imageFile=None, audioFile=None),
        ]
    )

    create_images(project)


if __name__ == "__main__":
    os.makedirs(GENERATIONS_DIR, exist_ok=True)
    main()
